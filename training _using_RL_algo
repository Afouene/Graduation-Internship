from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
from setting_the_environment import AUVEnvironment  
from stable_baselines3.common.env_util import make_vec_env

from stable_baselines3.common.sb2_compat.rmsprop_tf_like import RMSpropTFLike
import matplotlib.pyplot as plt
import os 
import numpy as np
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv

from vect import VecExtractDictObs
models_dir="training/PPO"
logdir="logs"
if not os.path.exists(models_dir):
    os.makedirs(models_dir)
if not os.path.exists(logdir):
    os.makedirs(logdir)  
def make_env():
    return AUVEnvironment()
env=AUVEnvironment()

model = PPO("MlpPolicy",env=env,  n_steps=100 ,verbose=0,gamma=0.99,tensorboard_log=logdir,batch_size=100,ent_coef=0.01,learning_rate=0.0003)
Timesteps=200000
model.learn(total_timesteps=Timesteps,progress_bar=True)
model.save(f"{models_dir}/{Timesteps}")
env.close()
